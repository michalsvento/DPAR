exp_name: "44k_8s"

trainer_callable: "training.trainer.Trainer"

lossmlp:
  use: True
  rff_dim: 32

finetuning: False
  

wandb:
  entity: "eloimoliner"
  project: "A-diffusion"

model_dir: None
#main options
#related to optimization
optimizer:
  type: "adam" #only "adam implemented
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8 #for numerical stability, we may need to modify it if usinf fp16
  
lr: 2e-4 #            help='Learning rate',
lr_rampup_it: 100 #,  help='Learning rate rampup duration'

#for lr scheduler (not noise schedule!!) TODO (I think)
scheduler_step_size: 60000
scheduler_gamma: 0.8


#save_model: True #wether to save the checkpoints of the model in this experiment

 
# Training related.
#total_its: 100000  #help='Training duration'
batch: 4 #         help='Total batch size'
batch_gpu: 4 #,     help='Limit batch size per GPU'

num_workers: 4  

# I/O-related. moved to logging
seed: 42 
resume: True
base_checkpoint: None
resume_checkpoint: None

#audio data related
sample_rate: 44100
audio_len: 262144
stereo: False #if False, the audio will be converted to mono by averaging the two channels


#training functionality parameters
device: "cpu" #it will be updated in the code, no worries

#training
use_cqt_DC_correction: False #if True, the loss will be corrected for the DC component and the nyquist frequency. This is important because we are discarding the DC component and the nyquist frequency in the cqt

#ema_rate: "0.9999"  # comma-separated list of EMA values
ema_rate: 0.9999  #unused
ema_rampup: 10000  #linear rampup to ema_rate   #help='EMA half-life' 


#gradient clipping
use_grad_clip: True
max_grad_norm: 1

restore : False
checkpoint_id: None


LTAS:
  calculate: True
  num_batches: 100
  nfft: 4096
  win_length: 4096
  hop_length: 1024
  Noct: 3


