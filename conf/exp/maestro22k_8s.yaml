exp_name: "22k_8s"

trainer_callable: "training.trainer.Trainer"

wandb:
  entity: "eloimoliner"
  project: "A-diffusion"

model_dir: None
#main options
#related to optimization
optimizer:
  type: "adam" #only "adam implemented
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8 #for numerical stability, we may need to modify it if usinf fp16
  
lr: 2e-4 #            help='Learning rate',
lr_rampup_it: 10000 #,  help='Learning rate rampup duration'

#for lr scheduler (not noise schedule!!) TODO (I think)
scheduler_step_size: 60000
scheduler_gamma: 0.8

lossmlp:
  use: False
  rff_dim: 32

finetuning: False



#save_model: True #wether to save the checkpoints of the model in this experiment

 
# Training related.
#total_its: 100000  #help='Training duration'
batch: 4 #         help='Total batch size'
batch_gpu: 4 #,     help='Limit batch size per GPU'
num_accumulation_rounds: 1 #gradient accumulation, truncated backprop


# Performance-related.
num_workers: 4  #',       help='DataLoader worker processes', metavar='INT',                 type=click.IntRange(min=1), default=1, show_default=True)

# I/O-related. moved to logging
seed: 42 #',          
resume: True
base_checkpoint: None
resume_checkpoint: None


#audio data related
sample_rate: 22050
audio_len: 184184
stereo: False #if False, the audio will be converted to mono by averaging the two channels


#training
use_cqt_DC_correction: False #if True, the loss will be corrected for the DC component and the nyquist frequency. This is important because we are discarding the DC component and the nyquist frequency in the cqt

ema_rate: 0.9999  #unused
ema_rampup: 10000  #linear rampup to ema_rate   #help='EMA half-life' 


#gradient clipping
use_grad_clip: True
max_grad_norm: 1

restore : False
checkpoint_id: None


LTAS:
  calculate: True
  num_batches: 100
  nfft: 4096
  win_length: 4096
  hop_length: 1024
  Noct: 3
